{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Failure Prediction - MLOps Experiment Tracking\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Iteration 1**: Baseline Logistic Regression model\n",
    "- **Iteration 2**: Improved Random Forest model\n",
    "- MLflow experiment tracking for both iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from CW1\n",
    "df = pd.read_csv('../data/processed/cleaned_hdd_from_faulty.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "print(\"Target distribution:\")\n",
    "print(df['failure'].value_counts())\n",
    "print(f\"\\nFailure rate: {df['failure'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Using the preprocessed features from CW1\n",
    "feature_columns = ['capacity_bytes', 'lifetime', 'model_encoded']\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['failure']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# Using random_state=42 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow\n",
    "# Connect to local MLflow server running in Docker\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create or set experiment\n",
    "experiment_name = \"hdd_failure_prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate and log metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return metrics dictionary\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_pred_proba\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, title):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(f'../reports/figures/{title.replace(\" \", \"_\").lower()}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return f'../reports/figures/{title.replace(\" \", \"_\").lower()}.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ITERATION 1: Baseline Logistic Regression Model\n",
    "\n",
    "For our baseline, we use Logistic Regression - a simple, interpretable model that works well for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATION 1: Logistic Regression Baseline\n",
    "with mlflow.start_run(run_name=\"iteration_1_logistic_regression\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"algorithm\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"features\", \",\".join(feature_columns))\n",
    "    mlflow.log_param(\"n_features\", len(feature_columns))\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"iteration\", 1)\n",
    "    \n",
    "    # Train model\n",
    "    model_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics_lr, y_pred_lr, y_pred_proba_lr = evaluate_model(model_lr, X_test, y_test)\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric_name, metric_value in metrics_lr.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_lr, \"model\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ITERATION 1: Logistic Regression Results\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric_name, metric_value in metrics_lr.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    run_id_lr = mlflow.active_run().info.run_id\n",
    "    print(f\"\\nMLflow Run ID: {run_id_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Iteration 1\n",
    "plot_confusion_matrix(y_test, y_pred_lr, \"Iteration 1 Logistic Regression Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ITERATION 2: Improved Random Forest Model\n",
    "\n",
    "For iteration 2, we use Random Forest - an ensemble method that typically provides better performance for complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATION 2: Random Forest (Improved Model)\n",
    "with mlflow.start_run(run_name=\"iteration_2_random_forest\"):\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    min_samples_split = 5\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.log_param(\"features\", \",\".join(feature_columns))\n",
    "    mlflow.log_param(\"n_features\", len(feature_columns))\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"iteration\", 2)\n",
    "    \n",
    "    # Train model\n",
    "    model_rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics_rf, y_pred_rf, y_pred_proba_rf = evaluate_model(model_rf, X_test, y_test)\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric_name, metric_value in metrics_rf.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_rf, \"model\")\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': model_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ITERATION 2: Random Forest Results\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric_name, metric_value in metrics_rf.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    run_id_rf = mlflow.active_run().info.run_id\n",
    "    print(f\"\\nMLflow Run ID: {run_id_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Iteration 2\n",
    "plot_confusion_matrix(y_test, y_pred_rf, \"Iteration 2 Random Forest Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both iterations\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(metrics_lr.keys()),\n",
    "    'Iteration 1 (Logistic Regression)': list(metrics_lr.values()),\n",
    "    'Iteration 2 (Random Forest)': list(metrics_rf.values())\n",
    "})\n",
    "comparison['Improvement'] = comparison['Iteration 2 (Random Forest)'] - comparison['Iteration 1 (Logistic Regression)']\n",
    "comparison['Improvement %'] = (comparison['Improvement'] / comparison['Iteration 1 (Logistic Regression)'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON: Iteration 1 vs Iteration 2\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['Iteration 1 (Logistic Regression)'], width, label='Iteration 1 (LR)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison['Iteration 2 (Random Forest)'], width, label='Iteration 2 (RF)', color='darkorange')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison: Iteration 1 vs Iteration 2')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Metric'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Register Best Model in MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model (Random Forest) in MLflow Model Registry\n",
    "model_name = \"hdd_failure_predictor\"\n",
    "\n",
    "# Register model from the Random Forest run\n",
    "model_uri = f\"runs:/{run_id_rf}/model\"\n",
    "registered_model = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model registered: {model_name}\")\n",
    "print(f\"Version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition model to Staging\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Transition to Staging\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=registered_model.version,\n",
    "    stage=\"Staging\"\n",
    ")\n",
    "\n",
    "print(f\"Model {model_name} version {registered_model.version} transitioned to Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Findings:\n",
    "- **Iteration 1 (Logistic Regression)**: Baseline model providing initial performance benchmarks\n",
    "- **Iteration 2 (Random Forest)**: Improved model with better handling of non-linear relationships\n",
    "\n",
    "### MLflow Tracking:\n",
    "- Both iterations logged to MLflow experiment: `hdd_failure_prediction`\n",
    "- Parameters, metrics, and models tracked for comparison\n",
    "- Best model registered in MLflow Model Registry\n",
    "\n",
    "### Next Steps:\n",
    "1. Export to production Python script\n",
    "2. Create testing scripts for reproducibility and performance regression\n",
    "3. Deploy model using MLflow serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nView experiments at: http://127.0.0.1:8080\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "print(f\"\\nIteration 1 Run ID: {run_id_lr}\")\n",
    "print(f\"Iteration 2 Run ID: {run_id_rf}\")\n",
    "print(f\"\\nRegistered Model: {model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
